{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Count Encodingのベースライン\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "\n",
    "import japanize_matplotlib\n",
    "import lightgbm as lgb\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import f1_score, roc_auc_score\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "warnings.simplefilter(\"ignore\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/nyk510/vivid/blob/master/vivid/utils.py\n",
    "from time import time\n",
    "\n",
    "def decorate(s: str, decoration=None):\n",
    "    if decoration is None:\n",
    "        decoration = '★' * 20\n",
    "\n",
    "    return ' '.join([decoration, str(s), decoration])\n",
    "\n",
    "class Timer:\n",
    "    def __init__(self, logger=None, format_str='{:.3f}[s]', prefix=None, suffix=None, sep=' ', verbose=0):\n",
    "\n",
    "        if prefix: format_str = str(prefix) + sep + format_str\n",
    "        if suffix: format_str = format_str + sep + str(suffix)\n",
    "        self.format_str = format_str\n",
    "        self.logger = logger\n",
    "        self.start = None\n",
    "        self.end = None\n",
    "        self.verbose = verbose\n",
    "\n",
    "    @property\n",
    "    def duration(self):\n",
    "        if self.end is None:\n",
    "            return 0\n",
    "        return self.end - self.start\n",
    "\n",
    "    def __enter__(self):\n",
    "        self.start = time()\n",
    "\n",
    "    def __exit__(self, exc_type, exc_val, exc_tb):\n",
    "        self.end = time()\n",
    "        if self.verbose is None:\n",
    "            return\n",
    "        out_str = self.format_str.format(self.duration)\n",
    "        if self.logger:\n",
    "            self.logger.info(out_str)\n",
    "        else:\n",
    "            print(out_str)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ROOT_DIR = Path('../')\n",
    "DATA_DIR = ROOT_DIR / 'data/input'\n",
    "OUTPUT_DIR = ROOT_DIR / 'data/outputs'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(DATA_DIR / Path(\"train.csv\"))\n",
    "test_df = pd.read_csv(DATA_DIR / Path(\"test.csv\"))\n",
    "era_df = pd.read_csv(DATA_DIR / Path(\"era.csv\"))\n",
    "\n",
    "display(train_df.head(5))\n",
    "display(test_df.head(5))\n",
    "display(era_df.head(5))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merge Data\n",
    "era_dfとtrain, testをmerge<br>\n",
    "最低限の前処理を行ってからmergeを行う。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "era_list = [\n",
    "    '鎌倉', '元', '古墳', '江戸', '高麗', '室町', '昭和', '大正', '唐', '桃山',\n",
    "    '奈良', '縄文', '南宋', '南北朝', '飛鳥', '平安', '北宋', '明', '明治', '弥生'\n",
    "]\n",
    "\n",
    "era_mapping = {}\n",
    "for era in era_list:\n",
    "    era_mapping[era + '前期'] = era\n",
    "    era_mapping[era + '中期'] = era\n",
    "    era_mapping[era + '後期'] = era\n",
    "    era_mapping[era + '末期'] = era\n",
    "    era_mapping[era + '時代'] = era\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_era(row):\n",
    "    if '～' in row['時代']:\n",
    "        if len(row['時代'].split('～'))>2:\n",
    "            start = row['時代'].split('～')[0]\n",
    "            end = row['時代'].split('～')[-1]\n",
    "        else:\n",
    "            start, end = row['時代'].split('～')\n",
    "        row['開始時代'] = start\n",
    "        row['終了時代'] = end\n",
    "    else:\n",
    "        row['開始時代'] = row['時代']\n",
    "        row['終了時代'] = row['時代']\n",
    "    return row\n",
    "\n",
    "\n",
    "def add_era(row, era_df):\n",
    "    if row['開始時代']==row['終了時代']:\n",
    "        return row\n",
    "    else:\n",
    "        if row['開始時代'] in era_df['時代'].values:\n",
    "            row['開始'] = era_df.loc[era_df['時代'] == row['開始時代'], '開始'].iloc[0]\n",
    "\n",
    "        if row['終了時代'] in era_df['時代'].values:\n",
    "            row['終了'] = era_df.loc[era_df['時代'] == row['終了時代'], '終了'].iloc[0]\n",
    "    return row\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df['時代'] = train_df['時代'].replace(era_mapping)\n",
    "test_df['時代'] = test_df['時代'].replace(era_mapping)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = train_df.merge(era_df, how=\"left\", on=\"時代\")\n",
    "test_df = test_df.merge(era_df, how=\"left\", on=\"時代\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df['時代'].fillna('不明', inplace=True)\n",
    "train_df = train_df.apply(split_era, axis=1)\n",
    "\n",
    "test_df['時代'].fillna('不明', inplace=True)\n",
    "test_df = test_df.apply(split_era, axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = train_df.apply(lambda row: add_era(row, era_df), axis=1)\n",
    "test_df = test_df.apply(lambda row: add_era(row, era_df), axis=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_split_prefecture(train: pd.DataFrame, test: pd.DataFrame) -> None:\n",
    "    train[['所有者住所', '所在都道府県']] = train['都道府県 ※美工品は「所有者住所（所在都道府県）」'].str.extract(r'([^\\（\\）]+)(?:（([^）]+)）)?')\n",
    "    test[['所有者住所', '所在都道府県']] = test['都道府県 ※美工品は「所有者住所（所在都道府県）」'].str.extract(r'([^\\（\\）]+)(?:（([^）]+)）)?')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "add_split_prefecture(train_df, test_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Count Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categoricals = [\"種別2\", \"所有者名\", '所有者住所', '所在都道府県']\n",
    "class AbstractBaseBlock:\n",
    "    def fit(self, input_df: pd.DataFrame, y=None):\n",
    "        return self.transform(input_df)\n",
    "\n",
    "    def transform(self, input_df: pd.DataFrame) -> pd.DataFrame:\n",
    "        raise NotImplementedError()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_whole_df():\n",
    "    return pd.concat([\n",
    "        train_df, test_df\n",
    "    ], ignore_index=True)\n",
    "\n",
    "class CountEncodingBlock(AbstractBaseBlock):\n",
    "    \"\"\"CountEncodingを行なう block\"\"\"\n",
    "    def __init__(self, column: str):\n",
    "        self.column = column\n",
    "\n",
    "    def fit(self, input_df, y=None):\n",
    "#         vc = input_df[self.column].value_counts()\n",
    "        master_df = read_whole_df()\n",
    "        vc = master_df[self.column].value_counts()\n",
    "        self.count_ = vc\n",
    "        return self.transform(input_df)\n",
    "\n",
    "    def transform(self, input_df):\n",
    "        out_df = pd.DataFrame()\n",
    "        out_df[self.column] = input_df[self.column].map(self.count_)\n",
    "        return out_df.add_prefix('CE_')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_blocks = [\n",
    "    *[CountEncodingBlock(c) for c in [\"種別2\", \"所有者名\", '所有者住所', '所在都道府県']]\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_blocks(input_df, blocks, y=None, test=False):\n",
    "    out_df = pd.DataFrame()\n",
    "\n",
    "    print(decorate('start run blocks...'))\n",
    "\n",
    "    with Timer(prefix='run test={}'.format(test)):\n",
    "        for block in feature_blocks:\n",
    "            with Timer(prefix='\\t- {}'.format(str(block))):\n",
    "                if not test:\n",
    "                    out_i = block.fit(input_df, y=y)\n",
    "                else:\n",
    "                    out_i = block.transform(input_df)\n",
    "\n",
    "            assert len(input_df) == len(out_i), block\n",
    "            name = block.__class__.__name__\n",
    "            out_df = pd.concat([out_df, out_i], axis=1)\n",
    "\n",
    "    return out_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ce_df = run_blocks(train_df, blocks=feature_blocks)\n",
    "test_ce_df = run_blocks(test_df, blocks=feature_blocks, test=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df_ = pd.merge(train_df, train_ce_df, left_index=True, right_index=True)\n",
    "test_df_ = pd.merge(test_df, test_ce_df, left_index=True, right_index=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = \"is_kokuhou\"\n",
    "\n",
    "features = [\"緯度\", \"経度\", \"開始\", \"終了\", 'CE_種別2', 'CE_所有者名', 'CE_所有者住所',\n",
    "       'CE_所在都道府県']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categoricals = ['CE_種別2', 'CE_所有者名', 'CE_所有者住所', 'CE_所在都道府県']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    \"n_estimators\": 50000,\n",
    "    \"boosting_type\": \"gbdt\",\n",
    "    \"metric\": \"auc\",\n",
    "    \"objective\": \"binary\",\n",
    "    \"n_jobs\": -1,\n",
    "    \"seed\": 42,\n",
    "    \"learning_rate\": 0.01,\n",
    "    \"verbose\": -1,\n",
    "}\n",
    "\n",
    "oof_pred = np.zeros(len(train_df))\n",
    "y_pred = np.zeros(len(test_df))\n",
    "models = []\n",
    "cv_scores = {}\n",
    "skf = StratifiedKFold(n_splits=5, random_state=42, shuffle=True)\n",
    "for fold, (train_index, test_index) in enumerate(\n",
    "    skf.split(train_df_[features], train_df_[target])\n",
    "):\n",
    "\n",
    "    print(f\"====== fold {fold} ======\")\n",
    "\n",
    "    x_train, x_val = (\n",
    "        train_df_.copy().iloc[train_index][features],\n",
    "        train_df_.copy().iloc[test_index][features],\n",
    "    )\n",
    "    y_train, y_val = (\n",
    "        train_df_.iloc[train_index][target],\n",
    "        train_df_.iloc[test_index][target],\n",
    "    )\n",
    "\n",
    "    test = test_df_[features]\n",
    "\n",
    "    # create Dataset\n",
    "    train_set = lgb.Dataset(\n",
    "        x_train, y_train, categorical_feature=categoricals, free_raw_data=False\n",
    "    )\n",
    "    val_set = lgb.Dataset(\n",
    "        x_val, y_val, categorical_feature=categoricals, free_raw_data=False\n",
    "    )\n",
    "\n",
    "    # train\n",
    "    verbose_eval = 100\n",
    "    model = lgb.train(\n",
    "        params,\n",
    "        train_set,\n",
    "        valid_sets=[train_set, val_set],\n",
    "        callbacks=[\n",
    "            lgb.early_stopping(\n",
    "                stopping_rounds=100,\n",
    "                verbose=True),\n",
    "            lgb.log_evaluation(verbose_eval)]\n",
    "    )\n",
    "\n",
    "    models.append(model)\n",
    "\n",
    "    fold_pred = model.predict(x_val)\n",
    "\n",
    "    score = roc_auc_score(y_val, fold_pred)\n",
    "    cv_scores[f\"cv{fold}\"] = score\n",
    "\n",
    "    oof_pred[test_index] = fold_pred\n",
    "\n",
    "    y_pred += model.predict(test) / 5\n",
    "\n",
    "    print(f\"cv score: {score}\")\n",
    "\n",
    "oof_score = roc_auc_score(train_df[target], oof_pred)\n",
    "print(f\"OOF score: {oof_score}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_importances = []\n",
    "\n",
    "for model in models:\n",
    "    feature_importances.append(model.feature_importance(importance_type=\"gain\"))\n",
    "\n",
    "feature_importances = np.array(feature_importances)\n",
    "feature_importance_df = pd.DataFrame(feature_importances, columns=features)\n",
    "sorted_features = feature_importance_df.median().sort_values(ascending=False).index\n",
    "sorted_feature_importance_df = feature_importance_df[sorted_features]\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.boxplot(data=sorted_feature_importance_df, orient=\"h\")\n",
    "plt.xlabel(\"Importance\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df[\"oof\"] = oof_pred\n",
    "sns.distplot(train_df[train_df[\"is_kokuhou\"] == 0][\"oof\"])\n",
    "sns.distplot(train_df[train_df[\"is_kokuhou\"] == 1][\"oof\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_optimal_threshold(y_true, y_scores):\n",
    "    thresholds = np.linspace(0, 1, 100)\n",
    "    best_threshold = 0\n",
    "    best_score = 0\n",
    "\n",
    "    for threshold in thresholds:\n",
    "        y_pred = (y_scores > threshold).astype(int)\n",
    "        score = f1_score(y_true, y_pred)\n",
    "\n",
    "        if score > best_score:\n",
    "            best_score = score\n",
    "            best_threshold = threshold\n",
    "\n",
    "    return best_threshold, best_score\n",
    "\n",
    "y_true = train_df['is_kokuhou'].values\n",
    "y_scores = oof_pred\n",
    "\n",
    "best_threshold, best_f1_score = find_optimal_threshold(y_true, y_scores)\n",
    "print(f\"Best threshold: {best_threshold}\") # 最適な閾値\n",
    "print(f\"Best F1 Score: {best_f1_score}\")\n",
    "\n",
    "\n",
    "y_pred = (y_pred > best_threshold).astype(int)\n",
    "print(y_pred)\n",
    "print(y_pred.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df[\"is_kokuhou\"] = y_pred\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df[[\"is_kokuhou\"]].to_csv(OUTPUT_DIR / \"submission_ver3_7_ce.csv\", index=False)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
